{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled40.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/NIS_SentiFrame/blob/master/virt_udpipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJCgjd0v5JKW"
      },
      "source": [
        "%%capture\n",
        "!pip install spacy_udpipe\n",
        "import spacy_udpipe"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqLqyFfUN8VK"
      },
      "source": [
        "%%capture\n",
        "!pip install pymorphy2[fast]\n",
        "from pymorphy2 import MorphAnalyzer \n",
        "morph = MorphAnalyzer()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oQleVfw4mDX"
      },
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8qTHP-11U3"
      },
      "source": [
        "# Sentence-level experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54GVtLN15oV"
      },
      "source": [
        "For further exploration of the semantic axis method we make use of BERT embeddings.\n",
        "\n",
        "Seed verbs remain the same as in the word-level experiments, but may be subject to change later.\n",
        "\n",
        "1.   We construct seed sentences with regard to the arguments of the test sentence. For example, to test a sentence like 'Силовики вломились к журналисту', we compute the semantic axis as follows:\n",
        "\n",
        "*   Replace the target verb with a seed verb.\n",
        "*   Make changes to cases of arguments if necessary.\n",
        "*   Repeat for each seed verb to construct seed sentences.\n",
        "\n",
        "2.   We embed sentences in the two seed groups and take averages of the two corresponding embedding groups.\n",
        "\n",
        "3.   We compute semantic axis for the test sentense by substracting the embedding of the negative seed from embedding of the positive seed.\n",
        "\n",
        "4.   We embed the test sentense and measure it's cosine similarity to the axis\n",
        "\n",
        "5.   We repeat previous steps for each test sentence with the predicate in question.\n",
        "\n",
        "6. The resulting dataset for all test sentenses will hopefully look like this:\n",
        "\n",
        "| Predicate(verb)      | Polarity | Text     | Similarity     |\n",
        "|    :----   |    :----   |    :----   |    ----:   |\n",
        "| защищать      | pos       | он защищает его   | 0,333       |\n",
        "| защищать      | pos       | суд защищает права   | 0,321       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn-sZhBpNkXW"
      },
      "source": [
        "# BERT-as-service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as7RU7qSOHLg"
      },
      "source": [
        "Check out [this issue](https://github.com/hanxiao/bert-as-service/issues/380) and \"make sure Colab is using Tensorflow 1.x, because bert-serving-start doesn't currently work with TF 2.1 and nohup hides the output of the command failing\".\n",
        "\n",
        "Also make sure you're using GPU accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acWmlZeroAC3",
        "outputId": "f80f7de6-fd84-4eb1-f92b-b3bedcacb6d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# import tensorflow as tf\n",
        "# print (tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5eP2_kNhvFf"
      },
      "source": [
        "%%capture\n",
        "!pip install -U bert-serving-server[http]\n",
        "!pip install bert-serving-client  # client, independent of `bert-serving-server`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gePhcxaviZj7"
      },
      "source": [
        "%%capture\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip /content/multi_cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pKKkhNVjhXH"
      },
      "source": [
        "!nohup bert-serving-start -model_dir=./multi_cased_L-12_H-768_A-12 > out.file 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW2AyjWJkZNz"
      },
      "source": [
        "from bert_serving.client import BertClient\n",
        "bc = BertClient()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLqDAKQ74QyR",
        "outputId": "a05b44fc-a589-4922-bde2-00982ee12c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "encoded_test = bc.encode(['First do it', \n",
        "                          'then do it right', 'then do it better'\n",
        "                          ])\n",
        "len(encoded_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnpR_O5BOW7C"
      },
      "source": [
        "# UDPipe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KuS_omTUZOY",
        "outputId": "f075d6d6-f005-4519-8890-bbdd9fd31548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spacy_udpipe.download(\"ru\")\n",
        "syntagrus = spacy_udpipe.load(\"ru\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded pre-trained UDPipe model for 'ru' language\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zp_ZjVC4olM"
      },
      "source": [
        "# with open('16.txt') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "# verbs = list(pd.read_csv('cross_seminar_task.csv', sep='\\t')['verb'])\n",
        "\n",
        "def text2ud(text):\n",
        "    udpiped = []\n",
        "    doc = syntagrus(text)\n",
        "    doc_len = len(doc)\n",
        "    for i, token in enumerate(doc):\n",
        "        if i <= 10 or i == doc_len-10:\n",
        "            continue\n",
        "        if token.lemma_ in verbs:\n",
        "            new_entry = {token.lemma_: []}\n",
        "            for t in reversed(doc[i-10:i]):\n",
        "                if t.head.lemma_ == token.lemma_:\n",
        "                    new_entry[token.lemma_].append([t.text, t.lemma_, t.pos_, t.dep_])\n",
        "            for t in doc[i:i+10]:\n",
        "                if t.head.lemma_ == token.lemma_:\n",
        "                    new_entry[token.lemma_].append([t.text, t.lemma_, t.pos_, t.dep_])\n",
        "            udpiped.append(new_entry)\n",
        "            \n",
        "    with open('result.json', 'w') as f:\n",
        "        json.dump(udpiped, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# text2ud(text[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZf1xPbYNNRY"
      },
      "source": [
        "# Semantic axis method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w5H8Ooy4va_"
      },
      "source": [
        "class SemanticAxis():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.seed0 = set()\n",
        "        self.seed1 = set()\n",
        "        self.targets = set()\n",
        "        self.axis_vector = None\n",
        "        self.axis_similarities = None\n",
        "\n",
        "    def add_seed(self, seed: set, seed_id: int):\n",
        "        if seed_id:\n",
        "            self.seed1 = set(seed)\n",
        "        else:\n",
        "            self.seed0 = set(seed)\n",
        "    def add2seed(self, seed: set, seed_id: int):\n",
        "        if seed_id:\n",
        "            self.seed1.update(seed)\n",
        "        else:\n",
        "            self.seed0.update(seed)\n",
        "    def flush_seed(self, seed_id=None, flush_both_seeds=True):\n",
        "        if seed_id != None:\n",
        "            if seed_id:\n",
        "                self.seed1 = set()\n",
        "            else:\n",
        "                self.seed0 = set()\n",
        "        else:\n",
        "            self.seed0, self.seed1 = set(), set()\n",
        "    \n",
        "    def add_targets(self, target):\n",
        "        self.targets = target\n",
        "    def add2targets(self, target):\n",
        "        self.targets.update(target)\n",
        "    def flush_targets(self):\n",
        "        self.targets = set()\n",
        "    \n",
        "    def compute_bert_axis(self, bert_client):\n",
        "        assert len(self.seed0) > 0, 'Seed0 set is empty.'\n",
        "        assert len(self.seed1) > 0, 'Seed1 set is empty.'\n",
        "        self.bert_client = bert_client\n",
        "\n",
        "        target_vectors = self.bert_client.encode(list(self.targets))\n",
        "        seed_vectors = [self.bert_client.encode(list(s)).mean(axis=0) \n",
        "        for s in (self.seed0, self.seed1)]\n",
        "\n",
        "        self.axis_vector = seed_vectors[1] - seed_vectors[0]\n",
        "\n",
        "        self.axis_similarities = {self.targets[i]:cosine_similarity(\n",
        "            np.atleast_2d(vector), \n",
        "            np.atleast_2d(self.axis_vector)\n",
        "            ).item() for i, vector in enumerate(target_vectors)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjou_zArlamn"
      },
      "source": [
        "sa = SemanticAxis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCG0gYJlipFY"
      },
      "source": [
        "sa.add_targets(list2check)\n",
        "sa.add_seed(['разрушать'], 0)\n",
        "sa.add_seed(['ценить'], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S1OBihYfekr"
      },
      "source": [
        "sa.compute_bert_axis(bc)\n",
        "df = pd.DataFrame({'target':sa.axis_similarities.keys(),\n",
        "                   'similarity':sa.axis_similarities.values()})\n",
        "df.sort_values(by='similarity')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gd_uF8B5LmY"
      },
      "source": [
        "# Triples (nsubj, root, obj)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVgwHN2jNhRD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woJnnZVDjibu"
      },
      "source": [
        "!unrar x \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN5WJqomlgll"
      },
      "source": [
        "def ovd_loader():\n",
        "    path = 'OVD-Info/2019'\n",
        "    for month in os.listdir(path):\n",
        "        for filename in os.listdir('{}/{}'.format(path, month)):\n",
        "            filepath = '{}/{}/{}'.format(path, month, filename)\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                yield filepath, f.read()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXsH-7IQnoo1"
      },
      "source": [
        "d = {x[0]:unicodedata.normalize(\"NFKD\", x[1]) for x in ovd_loader()}\n",
        "texts = pd.DataFrame({'url':d.keys(),\n",
        "                      'text':d.values()})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atd8iKY3AnqP",
        "outputId": "7cb7a75a-dbb1-4191-aff7-b8f1191ce0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "texts['spacy_doc'] = texts.text.progress_apply(syntagrus)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3073/3073 [17:20<00:00,  2.95it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUQP6n4FI-1C"
      },
      "source": [
        "# WIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCDfQIFJ3b9F"
      },
      "source": [
        "# переписать для spacy docs\n",
        "def triples_mapping(text, tags=tags, udpiper=syntagrus):\n",
        "    doc = udpiper(text)\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sent_triples = []\n",
        "        root_triple = defaultdict(list)\n",
        "        # переписать, оставить леммы и pos-теги\n",
        "        root_triple['root'] = sent.root\n",
        "        for child in sent.root.children:\n",
        "            if child.dep_ in tags:\n",
        "                root_triple[child.dep_].append(child)\n",
        "        sent_triples.append(dict(root_triple))\n",
        "    return sent_triples"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQqEf0RZ4Ip5",
        "outputId": "a253fbef-2764-4f35-f268-247d2c580f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# tqdm.pandas()\n",
        "# tags = 'nsubj obj iobj'.split()\n",
        "# texts['triples'] = texts.text.progress_apply(triples_mapping)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3073/3073 [17:13<00:00,  2.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtV9-6TwAHyx"
      },
      "source": [
        "pd.to_pickle(texts.spacy_doc, 'ovd-info_spacy_docs.pkl')"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ZbcWVqsP2D"
      },
      "source": [
        "triple_set = Counter()\n",
        "\n",
        "# переписать для колонки с spacy docs\n",
        "for text, text_triples in tripled.items():\n",
        "    for sent_triples in text_triples:\n",
        "        d = sent_triples[0]\n",
        "        # выкинули предложения с несколькими iobj\n",
        "        if 'iobj' in d:\n",
        "            if len(d['iobj']) > 1:\n",
        "                continue\n",
        "        # выкинули предложения с несколькими nsubj\n",
        "        if 'nsubj' in d:\n",
        "            if len(d['nsubj']) > 1:\n",
        "                continue\n",
        "        # переписать, потому что iobj остаются за бортом\n",
        "            else:\n",
        "                if 'obj' in d:\n",
        "        # для каждого дополнения\n",
        "                    for obj in d['obj']:\n",
        "                        tup = tuple(map(str, (d['nsubj'][0], d['root'], obj)))\n",
        "                        triple_set.update({tuple(t.lower() for t in tup)})"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgwbKERQ1YGf"
      },
      "source": [
        "triple_set.most_common(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcuzGBn22dz9"
      },
      "source": [
        "with open ('tripled_ovd-info_2019.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump({' '.join(list(tr)):v for tr, v in triple_set.items()}, \n",
        "        f, ensure_ascii=False)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrXfYGl-9rr0"
      },
      "source": [
        "predicates = {}\n",
        "predicates = defaultdict(list)\n",
        "for tr, v in triple_set.most_common():\n",
        "    for var in morph.parse(tr[1]):\n",
        "        if ('VERB' in var.tag) or ('INFN' in var.tag):\n",
        "            predicates[var.normal_form].append((tr, v))\n",
        "            continue"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY16zhZEFSNA"
      },
      "source": [
        "!wget 'https://github.com/DmitryKutsev/NIS_SentiFrame/raw/master/annotations/polarity_annotation/3annotators_agree_on_these.zip'\n",
        "!unzip '/content/3annotators_agree_on_these.zip'\n",
        "with open('annotated_negative_3annotators_agree.json')as f:\n",
        "    nverbs = set(json.load(f))\n",
        "with open('annotated_positive_3annotators_agree.json')as f:\n",
        "    pverbs = set(json.load(f))"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-a5iTilByp"
      },
      "source": [
        "p_multi_seed = ['одобрять', 'хвалить', 'поощрять', 'любить',\n",
        "                'обожать', 'восхищаться', 'восторгаться', 'гордиться',\n",
        "                'ценить','превозносить',]\n",
        "n_multi_seed = ['ненавидеть', 'ругать', 'злиться', 'порицать', \n",
        "                'осуждать', 'негодовать', 'обвинять', 'наказывать', \n",
        "                'убивать', 'разрушать',]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKVAsecFbpBj",
        "outputId": "6896221c-1ed0-4ba0-fa2e-23cb133fd0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_0 = 'избить задержать арестовать'.split()\n",
        "test_1 = 'поддержать оправдать защищать'.split()\n",
        "test_0 + test_1"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['избить', 'задержать', 'арестовать', 'поддержать', 'оправдать', 'защищать']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqI4a63Af5ys"
      },
      "source": [
        "test_triples = defaultdict(list)\n",
        "\n",
        "for v in test_0 + test_1:\n",
        "    for triple in predicates[v]:\n",
        "        test_triples[v].append(triple[0])"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgjCrXm0jacu"
      },
      "source": [
        "def (triple, seed0, seed1, morph=morph):\n",
        "    psents = []\n",
        "    n_sents = []\n",
        "    nsubj, root, obj = tuple(map(morph.parse, triple))\n",
        "    root.tag\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLN5Rxefn7mU"
      },
      "source": [
        "morph.parse('предстояло')[0].inflect({'pres', '3per'})\n",
        "dir(morph.parse('предстояло')[0].tag)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
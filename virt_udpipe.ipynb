{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled40.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryKutsev/NIS_SentiFrame/blob/master/virt_udpipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJCgjd0v5JKW"
      },
      "source": [
        "%%capture\n",
        "!pip install spacy_udpipe\n",
        "import spacy_udpipe"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqLqyFfUN8VK"
      },
      "source": [
        "%%capture\n",
        "!pip install pymorphy2[fast]\n",
        "from pymorphy2 import MorphAnalyzer \n",
        "morph = MorphAnalyzer()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oQleVfw4mDX"
      },
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8qTHP-11U3"
      },
      "source": [
        "# Sentence-level experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54GVtLN15oV"
      },
      "source": [
        "For further exploration of the semantic axis method we make use of BERT embeddings.\n",
        "\n",
        "Seed verbs remain the same as in the word-level experiments, but may be subject to change later.\n",
        "\n",
        "1.   We construct seed sentences with regard to the arguments of the test sentence. For example, to test a sentence like 'Силовики вломились к журналисту', we compute the semantic axis as follows:\n",
        "\n",
        "*   Replace the target verb with a seed verb.\n",
        "*   Make changes to cases of arguments if necessary.\n",
        "*   Repeat for each seed verb to construct seed sentences.\n",
        "\n",
        "2.   We embed sentences in the two seed groups and take averages of the two corresponding embedding groups.\n",
        "\n",
        "3.   We compute semantic axis for the test sentense by substracting the embedding of the negative seed from embedding of the positive seed.\n",
        "\n",
        "4.   We embed the test sentense and measure it's cosine similarity to the axis\n",
        "\n",
        "5.   We repeat previous steps for each test sentence with the predicate in question.\n",
        "\n",
        "6. The resulting dataset for all test sentenses will hopefully look like this:\n",
        "\n",
        "| Predicate(verb)      | Polarity | Text     | Similarity     |\n",
        "|    :----   |    :----   |    :----   |    ----:   |\n",
        "| защищать      | pos       | он защищает его   | 0,333       |\n",
        "| защищать      | pos       | суд защищает права   | 0,321       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn-sZhBpNkXW"
      },
      "source": [
        "# BERT-as-service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as7RU7qSOHLg"
      },
      "source": [
        "Check out [this issue](https://github.com/hanxiao/bert-as-service/issues/380) and \"make sure Colab is using Tensorflow 1.x, because bert-serving-start doesn't currently work with TF 2.1 and nohup hides the output of the command failing\".\n",
        "\n",
        "Also make sure you're using GPU accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acWmlZeroAC3",
        "outputId": "f80f7de6-fd84-4eb1-f92b-b3bedcacb6d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "# import tensorflow as tf\n",
        "# print (tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5eP2_kNhvFf"
      },
      "source": [
        "%%capture\n",
        "!pip install -U bert-serving-server[http]\n",
        "!pip install bert-serving-client  # client, independent of `bert-serving-server`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gePhcxaviZj7"
      },
      "source": [
        "%%capture\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip /content/multi_cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pKKkhNVjhXH"
      },
      "source": [
        "!nohup bert-serving-start -model_dir=./multi_cased_L-12_H-768_A-12 > out.file 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW2AyjWJkZNz"
      },
      "source": [
        "from bert_serving.client import BertClient\n",
        "bc = BertClient()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLqDAKQ74QyR",
        "outputId": "a05b44fc-a589-4922-bde2-00982ee12c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "encoded_test = bc.encode(['First do it', \n",
        "                          'then do it right', 'then do it better'\n",
        "                          ])\n",
        "len(encoded_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnpR_O5BOW7C"
      },
      "source": [
        "# UDPipe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KuS_omTUZOY",
        "outputId": "f075d6d6-f005-4519-8890-bbdd9fd31548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spacy_udpipe.download(\"ru\")\n",
        "syntagrus = spacy_udpipe.load(\"ru\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded pre-trained UDPipe model for 'ru' language\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zp_ZjVC4olM"
      },
      "source": [
        "# with open('16.txt') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "# verbs = list(pd.read_csv('cross_seminar_task.csv', sep='\\t')['verb'])\n",
        "\n",
        "def text2ud(text):\n",
        "    udpiped = []\n",
        "    doc = syntagrus(text)\n",
        "    doc_len = len(doc)\n",
        "    for i, token in enumerate(doc):\n",
        "        if i <= 10 or i == doc_len-10:\n",
        "            continue\n",
        "        if token.lemma_ in verbs:\n",
        "            new_entry = {token.lemma_: []}\n",
        "            for t in reversed(doc[i-10:i]):\n",
        "                if t.head.lemma_ == token.lemma_:\n",
        "                    new_entry[token.lemma_].append([t.text, t.lemma_, t.pos_, t.dep_])\n",
        "            for t in doc[i:i+10]:\n",
        "                if t.head.lemma_ == token.lemma_:\n",
        "                    new_entry[token.lemma_].append([t.text, t.lemma_, t.pos_, t.dep_])\n",
        "            udpiped.append(new_entry)\n",
        "            \n",
        "    with open('result.json', 'w') as f:\n",
        "        json.dump(udpiped, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# text2ud(text[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZf1xPbYNNRY"
      },
      "source": [
        "# Semantic axis method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w5H8Ooy4va_"
      },
      "source": [
        "class SemanticAxis():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.seed0 = set()\n",
        "        self.seed1 = set()\n",
        "        self.targets = set()\n",
        "        self.axis_vector = None\n",
        "        self.axis_similarities = None\n",
        "\n",
        "    def add_seed(self, seed: set, seed_id: int):\n",
        "        if seed_id:\n",
        "            self.seed1 = set(seed)\n",
        "        else:\n",
        "            self.seed0 = set(seed)\n",
        "    def add2seed(self, seed: set, seed_id: int):\n",
        "        if seed_id:\n",
        "            self.seed1.update(seed)\n",
        "        else:\n",
        "            self.seed0.update(seed)\n",
        "    def flush_seed(self, seed_id=None, flush_both_seeds=True):\n",
        "        if seed_id != None:\n",
        "            if seed_id:\n",
        "                self.seed1 = set()\n",
        "            else:\n",
        "                self.seed0 = set()\n",
        "        else:\n",
        "            self.seed0, self.seed1 = set(), set()\n",
        "    \n",
        "    def add_targets(self, target):\n",
        "        self.targets = target\n",
        "    def add2targets(self, target):\n",
        "        self.targets.update(target)\n",
        "    def flush_targets(self):\n",
        "        self.targets = set()\n",
        "    \n",
        "    def compute_bert_axis(self, bert_client):\n",
        "        assert len(self.seed0) > 0, 'Seed0 set is empty.'\n",
        "        assert len(self.seed1) > 0, 'Seed1 set is empty.'\n",
        "        self.bert_client = bert_client\n",
        "\n",
        "        target_vectors = self.bert_client.encode(list(self.targets))\n",
        "        seed_vectors = [self.bert_client.encode(list(s)).mean(axis=0) \n",
        "        for s in (self.seed0, self.seed1)]\n",
        "\n",
        "        self.axis_vector = seed_vectors[1] - seed_vectors[0]\n",
        "\n",
        "        self.axis_similarities = {self.targets[i]:cosine_similarity(\n",
        "            np.atleast_2d(vector), \n",
        "            np.atleast_2d(self.axis_vector)\n",
        "            ).item() for i, vector in enumerate(target_vectors)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjou_zArlamn"
      },
      "source": [
        "sa = SemanticAxis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCG0gYJlipFY"
      },
      "source": [
        "sa.add_targets(list2check)\n",
        "sa.add_seed(['разрушать'], 0)\n",
        "sa.add_seed(['ценить'], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S1OBihYfekr"
      },
      "source": [
        "sa.compute_bert_axis(bc)\n",
        "df = pd.DataFrame({'target':sa.axis_similarities.keys(),\n",
        "                   'similarity':sa.axis_similarities.values()})\n",
        "df.sort_values(by='similarity')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gd_uF8B5LmY"
      },
      "source": [
        "# Triples (nsubj, root, obj)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVgwHN2jNhRD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woJnnZVDjibu"
      },
      "source": [
        "!unrar x \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN5WJqomlgll"
      },
      "source": [
        "def ovd_loader():\n",
        "    path = 'OVD-Info/2019'\n",
        "    for month in os.listdir(path):\n",
        "        for filename in os.listdir('{}/{}'.format(path, month)):\n",
        "            filepath = '{}/{}/{}'.format(path, month, filename)\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                yield filepath, f.read()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXsH-7IQnoo1"
      },
      "source": [
        "d = {x[0]:unicodedata.normalize(\"NFKD\", x[1]) for x in ovd_loader()}\n",
        "texts = pd.DataFrame({'url':d.keys(),\n",
        "                      'text':d.values()})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atd8iKY3AnqP",
        "outputId": "7cb7a75a-dbb1-4191-aff7-b8f1191ce0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "texts['spacy_doc'] = texts.text.progress_apply(syntagrus)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3073/3073 [17:20<00:00,  2.95it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUQP6n4FI-1C"
      },
      "source": [
        "# WIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCDfQIFJ3b9F"
      },
      "source": [
        "# переписать для spacy docs\n",
        "def triples_mapping(text, tags=tags, udpiper=syntagrus):\n",
        "    doc = udpiper(text)\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sent_triples = []\n",
        "        root_triple = defaultdict(list)\n",
        "        # переписать, оставить леммы и pos-теги\n",
        "        root_triple['root'] = sent.root\n",
        "        for child in sent.root.children:\n",
        "            if child.dep_ in tags:\n",
        "                root_triple[child.dep_].append(child)\n",
        "        sent_triples.append(dict(root_triple))\n",
        "    return sent_triples"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQqEf0RZ4Ip5",
        "outputId": "a253fbef-2764-4f35-f268-247d2c580f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# tqdm.pandas()\n",
        "# tags = 'nsubj obj iobj'.split()\n",
        "# texts['triples'] = texts.text.progress_apply(triples_mapping)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3073/3073 [17:13<00:00,  2.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtV9-6TwAHyx"
      },
      "source": [
        "pd.to_pickle(texts.spacy_doc, 'ovd-info_spacy_docs.pkl')"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ZbcWVqsP2D"
      },
      "source": [
        "triple_set = Counter()\n",
        "\n",
        "# переписать для колонки с spacy docs\n",
        "for text, text_triples in tripled.items():\n",
        "    for sent_triples in text_triples:\n",
        "        d = sent_triples[0]\n",
        "        # выкинули предложения с несколькими iobj\n",
        "        if 'iobj' in d:\n",
        "            if len(d['iobj']) > 1:\n",
        "                continue\n",
        "        # выкинули предложения с несколькими nsubj\n",
        "        if 'nsubj' in d:\n",
        "            if len(d['nsubj']) > 1:\n",
        "                continue\n",
        "        # переписать, потому что iobj остаются за бортом\n",
        "            else:\n",
        "                if 'obj' in d:\n",
        "        # для каждого дополнения\n",
        "                    for obj in d['obj']:\n",
        "                        tup = tuple(map(str, (d['nsubj'][0], d['root'], obj)))\n",
        "                        triple_set.update({tuple(t.lower() for t in tup)})"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgwbKERQ1YGf"
      },
      "source": [
        "triple_set.most_common(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcuzGBn22dz9"
      },
      "source": [
        "with open ('tripled_ovd-info_2019.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump({' '.join(list(tr)):v for tr, v in triple_set.items()}, \n",
        "        f, ensure_ascii=False)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrXfYGl-9rr0"
      },
      "source": [
        "predicates = {}\n",
        "predicates = defaultdict(list)\n",
        "for tr, v in triple_set.most_common():\n",
        "    for var in morph.parse(tr[1]):\n",
        "        if ('VERB' in var.tag) or ('INFN' in var.tag):\n",
        "            predicates[var.normal_form].append((tr, v))\n",
        "            continue"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY16zhZEFSNA"
      },
      "source": [
        "!wget 'https://github.com/DmitryKutsev/NIS_SentiFrame/raw/master/annotations/polarity_annotation/3annotators_agree_on_these.zip'\n",
        "!unzip '/content/3annotators_agree_on_these.zip'\n",
        "with open('annotated_negative_3annotators_agree.json')as f:\n",
        "    nverbs = set(json.load(f))\n",
        "with open('annotated_positive_3annotators_agree.json')as f:\n",
        "    pverbs = set(json.load(f))"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-a5iTilByp"
      },
      "source": [
        "p_seed = ['одобрять', 'хвалить', 'поощрять', 'любить',\n",
        "                'обожать', 'ценить','превозносить', \n",
        "                # 'восхищаться', 'восторгаться', 'гордиться',\n",
        "                # added\n",
        "                'уважать'\n",
        "                ]\n",
        "n_seed = ['ненавидеть', 'ругать', 'убивать', 'разрушать',\n",
        "        #   'злиться', 'негодовать', \n",
        "          'порицать', 'осуждать', 'обвинять', 'наказывать', ]"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKVAsecFbpBj",
        "outputId": "4695bd71-b1c6-43f6-a152-012a062f3107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_0 = 'избить задержать арестовать'.split()\n",
        "test_1 = 'поддержать оправдать защищать'.split()\n",
        "test_0 + test_1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['избить', 'задержать', 'арестовать', 'поддержать', 'оправдать', 'защищать']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqI4a63Af5ys"
      },
      "source": [
        "test_triples = defaultdict(list)\n",
        "\n",
        "for v in test_0 + test_1:\n",
        "    for triple in predicates[v]:\n",
        "        test_triples[v].append(triple[0])"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10YyRCeomLPZ"
      },
      "source": [
        "#  пока без предлогов\n",
        "\n",
        "# объект, из которого будем доставать \n",
        "# сидовые предложения и тестовое предложение \n",
        "# для каждой тройки (a1, глагол, a2)\n",
        "class SentSeeds():\n",
        "\n",
        "    def __init__(self, triple, seed0, seed1, morph,\n",
        "                 casefile = 'cases.json',\n",
        "                #  прописать логирование\n",
        "                 logfile='SemanticAxis_log.txt'):\n",
        "        \n",
        "        self.triple = triple\n",
        "        self.morph = morph\n",
        "        self.casefile = casefile\n",
        "        self.logfile = logfile\n",
        "\n",
        "        self.seed_cases = self.read_seed_cases()\n",
        "        self.parsed_triple = self.parse_triple()\n",
        "        self.capitals = self.capitalization_check()\n",
        "        self.grammemes2inflect = self.extract_grammemes()\n",
        "\n",
        "        self.seed0 = seed0\n",
        "        self.seed1 = seed1\n",
        "\n",
        "        self.seed0_sents = self.stitch_seed(self.seed0)\n",
        "        self.seed1_sents = self.stitch_seed(self.seed1)\n",
        "\n",
        "    def read_seed_cases(self):\n",
        "        # нужен словарь со словарями для каждого сидового глагола\n",
        "        # словари возвращают падежи аргументов по аргументу\n",
        "        with open(self.casefile, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def parse_triple(self):\n",
        "        # морфологический разбор кортежа\n",
        "        a0, verb, a1 = tuple(map(self.morph.parse, self.triple))\n",
        "        # выбираем разбор наиболее частотных омографов\n",
        "        return a0[0], verb[0], a1[0]\n",
        "        \n",
        "    def capitalization_check(self):\n",
        "        return tuple([1 if el[0].isupper() else 0 for el in self.triple])\n",
        "\n",
        "    def inflect_sent(self, seed_verb):\n",
        "        # аргументы уже парсили\n",
        "        a0, a1 = self.parsed_triple[0], self.parsed_triple[2]\n",
        "        # сидовый глагол ещё не парсили\n",
        "        v = self.morph.parse(seed_verb)[0]\n",
        "        # граммемы, которыми будем приводить в нужную форму\n",
        "        grs = self.grammemes2inflect\n",
        "        # debug\n",
        "        # print(self.grammemes2inflect[2])\n",
        "        verb = v.inflect(grs[1]).word\n",
        "\n",
        "        d = self.seed_cases\n",
        "        # здесь должен быть 'nomn' в большинстве случаев\n",
        "        a0 = a0.inflect({d[seed_verb]['a0']}).word\n",
        "        # а здесь может быть что угодно\n",
        "        a1 = a1.inflect({d[seed_verb]['a1']}).word\n",
        "\n",
        "        return tuple([word.capitalize() if self.capitals[i] else word \n",
        "                for i, word in enumerate([a0, verb, a1])])\n",
        "\n",
        "    def stitch_seed(self, seed):\n",
        "        # добавляем в сиды предложения без знака конца предложения\n",
        "        return tuple([' '.join(self.inflect_sent(v)) for v in seed])\n",
        "        \n",
        "\n",
        "    def extract_grammemes(self):\n",
        "        a0, verb, a1 = self.parsed_triple\n",
        "        a0, verb, a1 = a0.tag, verb.tag, a1.tag\n",
        "        # лицо и род - взаимоисключающие граммемы в разборе глагола\n",
        "        grammemes = {g for g in (verb.tense, verb.number, \n",
        "                                 verb.gender, verb.person) if g}\n",
        "        # граммемы, которые можно использовать\n",
        "        # для постановки словв нужную форму\n",
        "        return tuple([{a0.case}, grammemes, {a1.case}])"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3B5W4v3guAB"
      },
      "source": [
        "x = SentSeeds(triple = ('Суд', 'приговорил', 'Елену'),\n",
        "              seed0=n_seed, seed1=p_seed+['нравиться'], morph=morph)"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLSoWnLPhUn7",
        "outputId": "eb12b60b-b27e-4c86-a029-c19741b9fa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "[sent for sent in x.seed1_sents]"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Суд одобрял Елену',\n",
              " 'Суд хвалил Елену',\n",
              " 'Суд поощрял Елену',\n",
              " 'Суд любил Елену',\n",
              " 'Суд обожал Елену',\n",
              " 'Суд ценил Елену',\n",
              " 'Суд превозносил Елену',\n",
              " 'Суд уважал Елену',\n",
              " 'Суд нравился Елене']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "365ov1yGf1EF"
      },
      "source": [
        "# Словари падежей аргументов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ4KN5Q6TWIP"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/DmitryKutsev/NIS_SentiFrame/master/annotations/SENTIFRAME%20-%20case_annotation.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFPPrdeOTf_3"
      },
      "source": [
        "casefile = pd.read_csv('SENTIFRAME - case_annotation.csv')"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGVvFVhbTqc_"
      },
      "source": [
        "casedict = {}\n",
        "casedict = defaultdict(dict)\n",
        "pol_col = casefile.polar_or_not.to_list()\n",
        "a0_col = casefile['падеж первого аргумента, по умолчанию nomn'].to_list()\n",
        "a1_col = casefile['падеж второго аргумента в нотации pymorphy2, по умолчанию accs'].to_list()\n",
        "\n",
        "preps = ['с', 'в', 'против', 'за', 'на', 'от', 'над', 'у', 'перед', 'из-за']\n",
        "\n",
        "for i, v in enumerate(casefile.verb.to_list()):\n",
        "    casedict[v].update({'polarity':pol_col[i]})\n",
        "    casedict[v].update({'a0':(a0_col[i] if pd.notna(a0_col[i]) else 'nomn')})\n",
        "    if pd.notna(a1_col[i]) and (a1_col[i] != '0'):\n",
        "        casedict[v].update({'a1':a1_col[i]})\n",
        "    prep = casefile['Комментарий'].to_list()[i]\n",
        "    if prep == 'c':\n",
        "        casedict[v].update({'preposition':'с'})\n",
        "    elif prep in preps:\n",
        "        casedict[v].update({'preposition':prep})"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp3eNMAgdyb0"
      },
      "source": [
        "# пока что None для неразмеченных полярностей\n",
        "casedict['хвалить'] = {'polarity': None, 'a0': 'nomn', 'a1': 'accs'}\n",
        "casedict['обожать'] = {'polarity': None, 'a0': 'nomn', 'a1': 'accs'}\n",
        "casedict['превозносить'] = {'polarity': None, 'a0': 'nomn', 'a1': 'accs'}\n",
        "casedict['ругать'] = {'polarity': None, 'a0': 'nomn', 'a1': 'accs'}\n",
        "casedict['порицать'] = {'polarity': None, 'a0': 'nomn', 'a1': 'accs'}\n",
        "casedict['наказывать'] = {'polarity': None, 'a0': 'nomn', 'a1': 'accs'}"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNCPeGKuZH_i"
      },
      "source": [
        "casedict = dict(casedict)\n",
        "with open('cases.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(casedict, f, ensure_ascii=False)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xdRM7O6TIQ-"
      },
      "source": [
        "# Каждый пример на своей оси"
      ]
    }
  ]
}